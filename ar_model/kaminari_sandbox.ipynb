{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30776,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os, gc, torchaudio, pydub, re\nfrom typing import Literal\nimport random\nimport wandb, datetime\nimport numpy as np\nimport torch\nfrom torch import nn, optim\nfrom torch.optim import lr_scheduler\nfrom accelerate import Accelerator, notebook_launcher\nfrom torch.cuda.amp import GradScaler\nfrom safetensors.torch import save_model\nfrom transformers import LlamaModel\nfrom time import time\nfrom tqdm.auto import tqdm\nfrom torch.utils.data import DataLoader, IterableDataset\nfrom datasets import load_dataset, Audio, Features\nfrom huggingface_hub import login\nfrom transformers import (\n    AutoTokenizer,\n    EncodecModel,\n    AutoProcessor,\n    LlamaModel,\n    LlamaConfig,\n    LlamaForCausalLM\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-04T22:10:01.464791Z","iopub.execute_input":"2024-10-04T22:10:01.465551Z","iopub.status.idle":"2024-10-04T22:10:23.502537Z","shell.execute_reply.started":"2024-10-04T22:10:01.465512Z","shell.execute_reply":"2024-10-04T22:10:23.501565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nhk = user_secrets.get_secret(\"hfkey\")\nwkey = user_secrets.get_secret(\"wandb\")\n\nlogin(hk)","metadata":{"execution":{"iopub.status.busy":"2024-10-04T22:10:23.504317Z","iopub.execute_input":"2024-10-04T22:10:23.505005Z","iopub.status.idle":"2024-10-04T22:10:23.853247Z","shell.execute_reply.started":"2024-10-04T22:10:23.504967Z","shell.execute_reply":"2024-10-04T22:10:23.852175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nclass config:\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    outpath = \"samples\"\n\n\nclass model_configs:\n    encodec_id = \"facebook/encodec_32khz\"\n    llama_id = \"meta-llama/Llama-3.2-1B\"\n    canary_id = \"tensorkelechi/kaminari_v1\"\n\n\nclass data_configs:\n    sample_rate = 32000\n    split = 4000\n    max_duration = 5\n    dtype = torch.float16\n    batch_size = 4\n    dataset_id = \"benjamin-paine/freesound-laion-640k\"\n    mini_dataset_id = \"lewtun/music_genres\"\n    #processed_repo_id = \"tensorkelechi/freesound_mini\"\n\n\nclass train_configs:\n    precision = torch.float16\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    grad_steps = 4\n    epochs = 1\n    lr = 1e-4\n    sft_file = 'kaminari.safetensors'\n    model_file = 'kaminari.pth'\n    outpath = 'kaminari'\n\nos.mkdir(config.outpath)","metadata":{"execution":{"iopub.status.busy":"2024-10-04T22:10:23.854479Z","iopub.execute_input":"2024-10-04T22:10:23.854841Z","iopub.status.idle":"2024-10-04T22:10:23.896946Z","shell.execute_reply.started":"2024-10-04T22:10:23.854785Z","shell.execute_reply":"2024-10-04T22:10:23.895771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nmusic_prefix = \"ðŸŽ¶\"\nstart_of_music = \"<somu>\"\nend_of_music = \"<eomu>\"\nmusic_codebook_size = 2048\nmusic_codebook_num = 4\nmusic_vocab_size = 8192\n\nmusic_tokens = {\n#     \"prefix\": music_prefix,\n    \"sos\": start_of_music,\n    \"eos\": end_of_music,\n}\n\n\ndef modality_tokens_to_string(tokens):\n    \"\"\"\n    Convert audio/music tokens to a single string with prefix and postfix.\n    \"\"\"\n    prefix = music_prefix\n    start = music_tokens[\"sos\"]\n    end = music_tokens[\"eos\"]\n\n    tokens_str = []\n    # music tokens are 2-dim array\n    # Convert each token to its corresponding string representation\n    for idx in range(len(tokens[0])):\n        for layer_idx in range(len(tokens)):\n            tokens_str.append(\n                f\"<{prefix}{tokens[layer_idx][idx] + music_codebook_size * layer_idx}>\"\n            )\n\n    tokens_string = \"\".join(tokens_str)\n    tokens_string = f\"{start}{tokens_string}{end}\"\n\n    return tokens_string\n","metadata":{"execution":{"iopub.status.busy":"2024-10-04T22:10:23.899940Z","iopub.execute_input":"2024-10-04T22:10:23.900653Z","iopub.status.idle":"2024-10-04T22:10:23.908900Z","shell.execute_reply.started":"2024-10-04T22:10:23.900603Z","shell.execute_reply":"2024-10-04T22:10:23.907910Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clear_mem():\n    torch.cuda.empty_cache()\n    gc.collect()\n\n\ndef trimpad_audio(audio):\n    samples = int(data_configs.sample_rate * data_configs.max_duration)\n#     audio = audio.numpy()\n\n    if len(audio) > samples:\n        audio = audio[:samples]\n\n    else:\n        pad_width = samples - len(audio)\n        audio = np.pad(audio, (0, pad_width), mode=\"reflect\")\n\n    return torch.as_tensor(audio)\n\n\ndef seed_everything(seed=33):\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n\nseed_everything()","metadata":{"execution":{"iopub.status.busy":"2024-10-04T22:10:23.910121Z","iopub.execute_input":"2024-10-04T22:10:23.910462Z","iopub.status.idle":"2024-10-04T22:10:23.930566Z","shell.execute_reply.started":"2024-10-04T22:10:23.910426Z","shell.execute_reply":"2024-10-04T22:10:23.929418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nCode for audio/music tokenization and model processing\n\"\"\"\ndef prepare_tokenizer(tokenizer, tokens: list):\n    special_tokens = [f\"<{music_prefix}{x}>\" for x in range(music_vocab_size)]\n    tokenizer.add_tokens(special_tokens)\n    tokenizer.add_tokens(tokens)\n    tokenizer.add_special_tokens({'pad_token': '[pad]'})\n    \n    return tokenizer\n\n\n# encoding/compressing music/audio waveform to tokens\ndef encode_music(audio, encodec_model, audio_processor):\n    \n    audio_array = trimpad_audio(audio)\n    \n    audio_proc = audio_processor(\n        raw_audio=audio_array, sampling_rate=data_configs.sample_rate,   \n        return_tensors='pt'\n    )  # preprocess audio waveform for encoding\n#     print(len(audio_proc[\"input_values\"]))\n#     print(len(audio_proc['padding_mask']))\n    \n    masks = audio_proc[\"padding_mask\"]  # get processor masks for decoding\n\n    with torch.no_grad():\n        audio_tokens = encodec_model.encode(\n            # tokenize/encode with pretrained neural codec\n            audio_proc[\"input_values\"],\n            audio_proc[\"padding_mask\"],\n        )\n    audio_codes = audio_tokens.audio_codes\n#     print(f'audio_codes.shape ={audio_codes.shape}')\n    \n    return audio_codes[0][0], masks\n\n\ndef tokens2string(tokens):\n    \"\"\"\n    Convert visual tokens to a single string with prefix and postfix.\n    \"\"\"\n    prefix = music_prefix\n    start = music_tokens[\"sos\"]\n    end = music_tokens[\"eos\"]\n\n    # music tokens are 2-dim array\n    # Convert each token to its corresponding string representation\n    tokens_str = []\n\n    for idx in range(len(tokens[0])):\n        #         print('layer 1')\n\n        for layer_idx in range(len(tokens)):\n            #             print('layer2')\n            tokens_str.append(\n                f\"<{prefix}{tokens[layer_idx][idx] + music_codebook_size * layer_idx}>\"\n            )\n\n    tokens_string = \"\".join(tokens_str)\n    tokens_string = f\" - {start}{tokens_string}{end}\"\n    return tokens_string\n\n\n\ndef extractor2(text, tag1=start_of_music, tag2=end_of_music):\n    start = None\n    try:\n        # print(text)\n        start = text.index(tag1) + len(tag1)\n        end = text.index(tag2, start)\n        extracted_text = text[start:end].strip()\n        if not extracted_text:\n            try:\n                extracted_text = text[start:]\n            except:\n                extracted_text = text\n        return extracted_text\n    except ValueError:\n        try:\n            extracted_text = text[start:]\n        except Exception as e:\n            print(e)\n            extracted_text = text\n        return extracted_text\n\ndef extract_content_between_final_tags(text, tag1=start_of_music, tag2=end_of_music):\n    \"\"\"\n     The content between the last occurrence of tag1 and tag2. Returns an empty string if tags are not found in order.\n    \"\"\"\n    last_tag1 = text.rfind(tag1)\n    last_tag2 = text.rfind(tag2)\n\n    if last_tag1 == -1 or last_tag2 == -1 or last_tag1 > last_tag2:\n        return None\n\n    # Extracting the content between the two tags\n    start = last_tag1 + len(tag1)\n    end = last_tag2\n    return text[start:end]\n\n\n# for audio decoding\ndef content2rvq_codes(content, codebook_size=2048, codebook_num=4):\n    codes = [int(code) for code in re.findall(r\"\\d+\", content)]\n#     print(len(codes))  # 6004\n    codes = np.array([code % codebook_size for code in codes])\n#     print(codes.shape)  # (6004,)\n    n = codes.shape[0] // codebook_num\n    print(n)  # (1501)\n    # Transpose the last two dimensions to match the desired output\n    # if can't divide evenly, drop the last few codes\n    codes = codes[: n * codebook_num]\n#     print(codes.shape)\n    codes = codes.reshape(n, codebook_num).T\n#     print(codes.shape)  # (4, 1501)\n    codes = np.expand_dims(codes, 0)\n    codes = np.expand_dims(codes, 0)\n#     print(codes.shape)  # (1, 1, 4, 1501)\n    codes = torch.tensor(codes).long().to(config.device)\n#     print(codes.shape)\n    return codes\n\n\ndef decode_music(content):\n    # codes = content2rvq_codes(content, music_codebook_size, music_codebook_num)\n    music = encodec_model.decode(content.cpu(), [None])\n#     print(f'decoded = {music}')\n    music = music[0].squeeze(0).detach().cpu()\n    print(f'decoded audio = {music.shape}')\n    return music","metadata":{"execution":{"iopub.status.busy":"2024-10-04T22:10:23.932061Z","iopub.execute_input":"2024-10-04T22:10:23.932382Z","iopub.status.idle":"2024-10-04T22:10:23.953406Z","shell.execute_reply.started":"2024-10-04T22:10:23.932349Z","shell.execute_reply":"2024-10-04T22:10:23.952367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# dataset preparation\n# for class-based music data, lewtun/music_genres\nmusic_data = load_dataset(\n    data_configs.dataset_id,\n    split=\"train\",\n    streaming=True,\n    trust_remote_code=True,\n).cast_column(\"audio\", Audio(sampling_rate=32000))\n\ndata_features = music_data.features.copy()\n\nmusic_data = music_data.map(\n    lambda r: {\"tags\": \" \".join(r[\"tags\"])}#, features=Features(data_features)\n)\n\nmusic_data = music_data.take(data_configs.split)\n\nmusic_data","metadata":{"execution":{"iopub.status.busy":"2024-10-04T22:10:23.954768Z","iopub.execute_input":"2024-10-04T22:10:23.955154Z","iopub.status.idle":"2024-10-04T22:10:31.051220Z","shell.execute_reply.started":"2024-10-04T22:10:23.955108Z","shell.execute_reply":"2024-10-04T22:10:31.049983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Audio encoder, FaceBook Encodec-32khz\nencodec_model = EncodecModel.from_pretrained(model_configs.encodec_id)\n\naudio_processor = AutoProcessor.from_pretrained(\n    model_configs.encodec_id\n)  # preprocessor for neural audio codec\n\n# freeze or prevent gradient update\nencodec_model=encodec_model.eval()\ntype(encodec_model)","metadata":{"execution":{"iopub.status.busy":"2024-10-04T22:10:31.052804Z","iopub.execute_input":"2024-10-04T22:10:31.053170Z","iopub.status.idle":"2024-10-04T22:10:34.135627Z","shell.execute_reply.started":"2024-10-04T22:10:31.053128Z","shell.execute_reply":"2024-10-04T22:10:34.134511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MusicData(IterableDataset):\n    def __init__(self, tokenizer, dataset=music_data):\n        self.dataset = dataset\n        self.tokenizer = tokenizer\n\n    def __len__(self):\n        return data_configs.split\n\n    def __iter__(self):\n        for sample in self.dataset:\n            audio_tokens = encode_music(\n                sample[\"audio\"][\"array\"],\n                encodec_model=encodec_model,\n                audio_processor=audio_processor,\n            )\n            audio_string = tokens2string(audio_tokens[0])\n\n            label = sample[\"tags\"]#' '.join(sample[\"tags\"])\n            data_string = label + audio_string\n\n            input_tokens = self.tokenizer(data_string, return_tensors='pt', truncation=True, padding='max_length', max_length=1024)\n            token_ids = input_tokens[\"input_ids\"]\n            attn_mask = input_tokens[\"attention_mask\"]\n\n            yield {\"input_ids\": token_ids, \"attention_mask\": attn_mask}","metadata":{"execution":{"iopub.status.busy":"2024-10-04T22:10:34.136893Z","iopub.execute_input":"2024-10-04T22:10:34.137210Z","iopub.status.idle":"2024-10-04T22:10:34.145639Z","shell.execute_reply.started":"2024-10-04T22:10:34.137177Z","shell.execute_reply":"2024-10-04T22:10:34.144555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import LlamaTokenizer\n\ntokenizer = LlamaTokenizer.from_pretrained(\"hf-internal-testing/llama-tokenizer\")\n\n# LLM tokenizer\n# tokenizer = AutoTokenizer.from_pretrained(\n# #     model_configs.llama_id,\n# )\n# # tokenizer = prepare_tokenizer(tokenizer)\n\n# Llama model architecture, for initial experiments\n\nllama_config = LlamaConfig(\n    num_attention_heads=16,\n    num_hidden_layers=24,\n    num_key_value_heads=8,\n    hidden_size=1024,\n    intermediate_size=4096\n)\n\ntiny_llama = LlamaModel(config=llama_config)\ntiny_llama.config","metadata":{"execution":{"iopub.status.busy":"2024-10-04T22:10:34.150664Z","iopub.execute_input":"2024-10-04T22:10:34.150985Z","iopub.status.idle":"2024-10-04T22:10:37.845597Z","shell.execute_reply.started":"2024-10-04T22:10:34.150952Z","shell.execute_reply":"2024-10-04T22:10:37.844279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokens = list(music_tokens.values())\n\ntokenizer = prepare_tokenizer(tokenizer, tokens)\ntiny_llama.resize_token_embeddings(len(tokenizer))\n\ntiny_llama.lm_head = nn.Linear(llama_config.hidden_size, len(tokenizer), bias=False)\n\ntype(tiny_llama), tiny_llama.config","metadata":{"execution":{"iopub.status.busy":"2024-10-04T22:10:37.846807Z","iopub.execute_input":"2024-10-04T22:10:37.847163Z","iopub.status.idle":"2024-10-04T22:10:39.853817Z","shell.execute_reply.started":"2024-10-04T22:10:37.847127Z","shell.execute_reply":"2024-10-04T22:10:39.852679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dset = MusicData(tokenizer)\nmini_train_loader = DataLoader(dataset=dset, batch_size=data_configs.batch_size)\n\n# x_sample = next(iter(mini_train_loader))\n# x_sample/","metadata":{"execution":{"iopub.status.busy":"2024-10-04T22:10:39.855067Z","iopub.execute_input":"2024-10-04T22:10:39.855405Z","iopub.status.idle":"2024-10-04T22:10:39.860615Z","shell.execute_reply.started":"2024-10-04T22:10:39.855370Z","shell.execute_reply":"2024-10-04T22:10:39.859366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# training definitions\nmodel = tiny_llama\n\nloss_fn = nn.CrossEntropyLoss(reduction=\"none\", ignore_index=tokenizer.pad_token_id)  # loss function\noptimizer = optim.AdamW(model.parameters(), lr=train_configs.lr)\nscheduler = lr_scheduler.CosineAnnealingWarmRestarts(\n    optimizer,\n    T_0=1000,  # restart every 1000 steps\n    T_mult=1\n)\n\nscaler = GradScaler()\n\n# configure accelerate\n# accelerator = Accelerator()\n# model, mini_train_loader, optimizer, scheduler = accelerator.prepare(\n#     # cofnigure modules for training\n#     model,\n#     mini_train_loader,\n#     optimizer,\n#     scheduler,\n# )","metadata":{"execution":{"iopub.status.busy":"2024-10-04T22:10:39.862132Z","iopub.execute_input":"2024-10-04T22:10:39.862495Z","iopub.status.idle":"2024-10-04T22:10:39.874621Z","shell.execute_reply.started":"2024-10-04T22:10:39.862452Z","shell.execute_reply":"2024-10-04T22:10:39.873646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def _postprocess(input):\n    extract = extractor2(input)\n    reconstruct_codes = content2rvq_codes(extract)\n    print(f'recoded {reconstruct_codes.shape}')\n    waveform = decode_music(reconstruct_codes)\n\n    waveform = waveform[0].squeeze(0).detach().cpu()\n\n    return waveform\n\n\n@torch.no_grad()\ndef bird_call(\n    prompt, model, tokenizer=tokenizer\n):  # prompt might be just a class/single word/description for v1\n    \n    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding='max_length', max_length=1024)\n    \n    input_ids = inputs['input_ids'].to(config.device)\n    attn_mask = inputs['attention_mask'].to(config.device)\n    \n    gen_tokens = model(\n        input_ids=input_ids, \n        attention_mask=attn_mask\n    )[0]\n    \n    gen_tokens = model.lm_head(gen_tokens)\n    gen_tokens = gen_tokens.argmax(dim=-1)[0]\n\n    tokens = tokenizer.decode(gen_tokens.cpu(), skip_special_tokens=True)\n    print(tokens)\n    output = _postprocess(tokens)\n    print(f'postprocessed: {output}')\n    \n    return output","metadata":{"execution":{"iopub.status.busy":"2024-10-04T22:10:39.876009Z","iopub.execute_input":"2024-10-04T22:10:39.876460Z","iopub.status.idle":"2024-10-04T22:10:39.887131Z","shell.execute_reply.started":"2024-10-04T22:10:39.876397Z","shell.execute_reply":"2024-10-04T22:10:39.885986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# k = bird_call('classical', model)","metadata":{"execution":{"iopub.status.busy":"2024-10-04T22:10:39.888438Z","iopub.execute_input":"2024-10-04T22:10:39.889042Z","iopub.status.idle":"2024-10-04T22:10:39.900410Z","shell.execute_reply.started":"2024-10-04T22:10:39.888998Z","shell.execute_reply":"2024-10-04T22:10:39.899589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import soundfile as sf\n\ndef count_params(model: nn.Module):\n    p_count = sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n    return p_count\n\n\nprint(f\"model parameters (training) = {count_params(model)}\")\n\ndef clearmem():\n    torch.cuda.empty_cache()\n    gc.collect()\n\ndef logger(model) -> None:\n    wandb.login(key=wkey)\n    wandb.init(project=\"kaminari_v1\", name=\"audiogen-sandbox-5\")\n    wandb.watch(model)\n\nlogger(model)\n\n\n@torch.no_grad\ndef epoch_sample(model: LlamaModel = model, prompt=\"classical\"):\n    sample_tokens = bird_call(prompt, model, tokenizer)\n    sample_numpy = sample_tokens.cpu().numpy().astype(np.float32)\n    print(sample_numpy.shape)\n    print(sample_numpy.dtype)\n    \n    now = datetime.datetime.now()\n    filename = now.strftime(\"%m%d_%H%M%S\") + \".wav\"\n    file_name = os.path.join(config.outpath, filename)\n    \n    sf.write(file_name, sample_numpy, data_configs.sample_rate)\n#     torchaudio.save(file_name, sample_tokens, data_configs.sample_rate)#, channels_first=True)\n    print(\"saved: \", file_name)\n\n    return os.path.join(config.outpath, filename)","metadata":{"execution":{"iopub.status.busy":"2024-10-04T22:10:39.901679Z","iopub.execute_input":"2024-10-04T22:10:39.902360Z","iopub.status.idle":"2024-10-04T22:10:42.879472Z","shell.execute_reply.started":"2024-10-04T22:10:39.902315Z","shell.execute_reply":"2024-10-04T22:10:42.878536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from IPython import display as idp\n\n# file = epoch_sample()\n\n# file","metadata":{"execution":{"iopub.status.busy":"2024-10-04T22:10:42.880825Z","iopub.execute_input":"2024-10-04T22:10:42.881289Z","iopub.status.idle":"2024-10-04T22:10:42.886519Z","shell.execute_reply.started":"2024-10-04T22:10:42.881231Z","shell.execute_reply":"2024-10-04T22:10:42.885446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# idp.Audio(filename=file, rate=32000)","metadata":{"execution":{"iopub.status.busy":"2024-10-04T22:10:42.888165Z","iopub.execute_input":"2024-10-04T22:10:42.889057Z","iopub.status.idle":"2024-10-04T22:10:42.900709Z","shell.execute_reply.started":"2024-10-04T22:10:42.889007Z","shell.execute_reply":"2024-10-04T22:10:42.899615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clearmem()","metadata":{"execution":{"iopub.status.busy":"2024-10-04T22:10:42.902528Z","iopub.execute_input":"2024-10-04T22:10:42.902943Z","iopub.status.idle":"2024-10-04T22:10:43.252155Z","shell.execute_reply.started":"2024-10-04T22:10:42.902895Z","shell.execute_reply":"2024-10-04T22:10:43.251141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CUDA_LAUNCH_BLOCKING=1\nTORCH_USE_CUDA_DSA=True\n\nimport torch._dynamo\ntorch._dynamo.config.suppress_errors = True","metadata":{"execution":{"iopub.status.busy":"2024-10-04T22:10:43.253501Z","iopub.execute_input":"2024-10-04T22:10:43.253856Z","iopub.status.idle":"2024-10-04T22:10:43.262033Z","shell.execute_reply.started":"2024-10-04T22:10:43.253818Z","shell.execute_reply":"2024-10-04T22:10:43.261150Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model=model.to(train_configs.device)","metadata":{"execution":{"iopub.status.busy":"2024-10-04T22:10:43.263315Z","iopub.execute_input":"2024-10-04T22:10:43.263669Z","iopub.status.idle":"2024-10-04T22:10:43.273025Z","shell.execute_reply.started":"2024-10-04T22:10:43.263634Z","shell.execute_reply":"2024-10-04T22:10:43.272031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.resize_token_embeddings(len(tokenizer))\nmodel.config.vocab_size = len(tokenizer)\n\nlen(tokenizer)","metadata":{"execution":{"iopub.status.busy":"2024-10-04T22:10:43.293228Z","iopub.execute_input":"2024-10-04T22:10:43.293633Z","iopub.status.idle":"2024-10-04T22:10:43.305604Z","shell.execute_reply.started":"2024-10-04T22:10:43.293584Z","shell.execute_reply":"2024-10-04T22:10:43.304658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer.pad_token_id","metadata":{"execution":{"iopub.status.busy":"2024-10-04T22:10:43.306785Z","iopub.execute_input":"2024-10-04T22:10:43.307086Z","iopub.status.idle":"2024-10-04T22:10:43.317658Z","shell.execute_reply.started":"2024-10-04T22:10:43.307053Z","shell.execute_reply":"2024-10-04T22:10:43.316567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Tokenizer vocabulary size: {len(tokenizer)}\")\nprint(f\"Model embedding size: {model.embed_tokens.num_embeddings}\")\nprint(f\"Model config vocab size: {model.config.vocab_size}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-04T22:10:43.318990Z","iopub.execute_input":"2024-10-04T22:10:43.319415Z","iopub.status.idle":"2024-10-04T22:10:43.328680Z","shell.execute_reply.started":"2024-10-04T22:10:43.319374Z","shell.execute_reply":"2024-10-04T22:10:43.327347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def trainer(\n    model=model, train_loader=mini_train_loader, epoch_count=train_configs.epochs\n):\n    model.train()\n    model.to(config.device)\n\n    train_loss = 0.0\n    # training loop\n    for epoch in tqdm(range(epoch_count)):\n        print(f'training for epoch {epoch+1}')\n        start_time = time()\n        optimizer.zero_grad()  # clear gradient graph\n\n        for step, batch in tqdm(enumerate(train_loader)):\n            optimizer.zero_grad()  # clear gradient graph\n\n            input_tokens = batch[\"input_ids\"].to(config.device)\n            attn_mask = batch[\"attention_mask\"].to(config.device)\n\n            assert input_tokens.max() < model.config.vocab_size, f\"Input contains token ID {input_tokens.max().item()} which is >= vocab size {model.config.vocab_size}\"\n            # Mixed precision training\n            with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n                outputs = model(\n                    input_ids=input_tokens.long().squeeze(),  # .squeeze(),\n                    attention_mask=attn_mask.long().squeeze(),  # .squeeze(),\n#                     labels=input_tokens.long().squeeze(),\n                )[0]\n                outputs = model.lm_head(outputs)\n\n                # clear memory\n                clearmem()\n\n                # slice tensors, due to 'next-token prediction' objective\n                # all except last token\n                output_tensor = outputs[..., :-1, :].contiguous()\n                # all except the first token\n                targets = input_tokens[..., 1:].contiguous()\n                shift_mask = attn_mask[..., 1:].contiguous()\n\n                model_output = output_tensor.view(-1, output_tensor.size(-1))\n                targets = targets.view(-1)\n\n                # compute loss for step\n                step_loss = loss_fn(model_output, targets)\n                clearmem()\n                \n                total_tokens = shift_mask.sum()\n                step_loss = step_loss.sum() / (total_tokens + 1e-8)\n                \n                # Scale loss by accumulation steps\n                train_loss = step_loss / train_configs.grad_steps  # Normalize the loss\n                \n                print(f\"step {step}: loss {step_loss:.4f}\")\n                wandb.log({\"step_loss\": step_loss})\n                \n                clearmem()                \n            # optimizer.step()\n\n            # Scales loss. Calls backward() on scaled loss to create scaled gradients.\n            scaler.scale(train_loss).backward()\n\n            clearmem()\n\n            if (step + 1) % train_configs.grad_steps == 0:\n                # Unscales the gradients of optimizer's assigned params in-place\n                scaler.step(optimizer)\n                # Updates the scale for next iteration\n                scaler.update()\n                optimizer.zero_grad()\n\n            if step % 5 == 0:\n                wandb.log({\"train_loss\": train_loss})\n            \n            if (step % 500) == 0:\n                checkpoint = {\n                    \"epoch\": epoch,\n                    \"model_state_dict\": model.state_dict(),\n                    \"optimizer_state_dict\": optimizer.state_dict(),\n                    \"scheduler_state\": scheduler.state_dict(),\n                    \"loss\": train_loss,\n                }\n\n                # save checkpoint\n                torch.save(checkpoint, f\"kaminari_mini_check_{epoch}.pth\")\n            \n            if (step % 50) == 0:\n                # log audio sample to WandB\n                try:\n                    test_sample_file = epoch_sample(model)\n                    wandb.log(\n                        {\n                            \"audio_sample\": wandb.Audio(\n                                test_sample_file,\n                                caption=f\"test_audio_track_{step}\",\n                                sample_rate=data_configs.sample_rate,\n                            )\n                        }\n                    )\n                except Exception as e:\n                    print(f'error logging sample: {e}')\n                \n#         scheduler.step()\n\n        gc.collect()\n        epoch_time = time() - start_time\n\n        print(f\"Epoch {epoch} of {epoch_count}, train_loss: {train_loss:.4f}\")\n\n        print(f\"Epoch @ {epoch} complete in {epoch_time}!\")\n\n    print(f\"End metrics for run of {epoch_count}, train_loss: {train_loss:.4f}\")\n\n    save_model(model, train_configs.sft_file)  # save to .safetensors file\n    checkpoint = {\n                    \"model_state_dict\": model.state_dict(),\n                    \"optimizer_state_dict\": optimizer.state_dict(),\n#                     \"scheduler_state\": scheduler.state_dict(),\n                    \"loss\": train_loss,\n                }\n    torch.save(checkpoint, f\"check_{train_configs.model_file}\")\n    \n    torch.save(model.state_dict(), f\"{train_configs.model_file}\")\n    \n    return model\n    \nmodel = trainer()","metadata":{"execution":{"iopub.status.busy":"2024-10-04T22:10:43.330252Z","iopub.execute_input":"2024-10-04T22:10:43.330674Z","iopub.status.idle":"2024-10-04T22:18:21.409079Z","shell.execute_reply.started":"2024-10-04T22:10:43.330641Z","shell.execute_reply":"2024-10-04T22:18:21.406819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save_pretrained('kaminari_v1')\nmodel.push_to_hub('tensorkelechi/kaminari_v1')\ntokenizer.push_to_hub('kaminari_v1')","metadata":{"execution":{"iopub.status.busy":"2024-10-04T22:18:21.410008Z","iopub.status.idle":"2024-10-04T22:18:21.410416Z","shell.execute_reply.started":"2024-10-04T22:18:21.410221Z","shell.execute_reply":"2024-10-04T22:18:21.410241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loaded_model = model.from_pretrained('tensorkelechi/kaminari_v1')\nloaded_model","metadata":{"execution":{"iopub.status.busy":"2024-10-04T22:18:21.411423Z","iopub.status.idle":"2024-10-04T22:18:21.411809Z","shell.execute_reply.started":"2024-10-04T22:18:21.411610Z","shell.execute_reply":"2024-10-04T22:18:21.411630Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clearmem()\nclearmem()","metadata":{"execution":{"iopub.status.busy":"2024-10-04T22:18:21.413658Z","iopub.status.idle":"2024-10-04T22:18:21.414020Z","shell.execute_reply.started":"2024-10-04T22:18:21.413840Z","shell.execute_reply":"2024-10-04T22:18:21.413859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import Audio\n\nmodel.to(config.device).eval()\n\ntest_sample = epoch_sample(model, 'sound, noise')\n\nAudio(test_sample)","metadata":{"execution":{"iopub.status.busy":"2024-10-04T22:18:21.416054Z","iopub.status.idle":"2024-10-04T22:18:21.416593Z","shell.execute_reply.started":"2024-10-04T22:18:21.416322Z","shell.execute_reply":"2024-10-04T22:18:21.416349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_sample2 = epoch_sample(model, 'instrumental')\n\nAudio(test_sample2)","metadata":{"execution":{"iopub.status.busy":"2024-10-04T22:18:21.417848Z","iopub.status.idle":"2024-10-04T22:18:21.418400Z","shell.execute_reply.started":"2024-10-04T22:18:21.418133Z","shell.execute_reply":"2024-10-04T22:18:21.418160Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer.push_to_hub('tensorkelechi/kaminari_v1')\n","metadata":{"execution":{"iopub.status.busy":"2024-10-04T22:18:21.419818Z","iopub.status.idle":"2024-10-04T22:18:21.420211Z","shell.execute_reply.started":"2024-10-04T22:18:21.419996Z","shell.execute_reply":"2024-10-04T22:18:21.420015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# def trainer_wrapper(train_function=):\n#     train_function()\n\n\n# notebook_launcher(trainer_wrapper, num_processes=2)","metadata":{"execution":{"iopub.status.busy":"2024-10-04T22:18:21.422014Z","iopub.status.idle":"2024-10-04T22:18:21.422427Z","shell.execute_reply.started":"2024-10-04T22:18:21.422219Z","shell.execute_reply":"2024-10-04T22:18:21.422238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clearmem()","metadata":{"execution":{"iopub.status.busy":"2024-10-04T22:18:21.423297Z","iopub.status.idle":"2024-10-04T22:18:21.423663Z","shell.execute_reply.started":"2024-10-04T22:18:21.423477Z","shell.execute_reply":"2024-10-04T22:18:21.423497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('kaminari training complete')","metadata":{"execution":{"iopub.status.busy":"2024-10-04T22:18:21.425152Z","iopub.status.idle":"2024-10-04T22:18:21.425511Z","shell.execute_reply.started":"2024-10-04T22:18:21.425332Z","shell.execute_reply":"2024-10-04T22:18:21.425350Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}