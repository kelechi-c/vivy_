{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30776,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os, gc, torchaudio, pydub, re\nfrom typing import Literal\nimport random\nimport wandb, datetime\nimport numpy as np\nimport torch\nfrom torch import nn, optim\nfrom torch.optim import lr_scheduler\nfrom accelerate import Accelerator, notebook_launcher\nfrom torch.cuda.amp import GradScaler\nfrom safetensors.torch import save_model\nfrom transformers import LlamaModel\nfrom time import time\nfrom tqdm.auto import tqdm\nfrom torch.utils.data import DataLoader, IterableDataset\nfrom datasets import load_dataset, Audio, Features\nfrom huggingface_hub import login\nfrom transformers import (\n    AutoTokenizer,\n    EncodecModel,\n    AutoProcessor,\n    LlamaModel,\n    LlamaConfig,\n    LlamaForCausalLM\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-03T23:21:16.557621Z","iopub.execute_input":"2024-10-03T23:21:16.558095Z","iopub.status.idle":"2024-10-03T23:21:41.651622Z","shell.execute_reply.started":"2024-10-03T23:21:16.558049Z","shell.execute_reply":"2024-10-03T23:21:41.650236Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nhk = user_secrets.get_secret(\"hfkey\")\nwkey = user_secrets.get_secret(\"wandb\")\n\nlogin(hk)","metadata":{"execution":{"iopub.status.busy":"2024-10-03T23:21:41.653965Z","iopub.execute_input":"2024-10-03T23:21:41.654707Z","iopub.status.idle":"2024-10-03T23:21:42.287919Z","shell.execute_reply.started":"2024-10-03T23:21:41.654660Z","shell.execute_reply":"2024-10-03T23:21:42.286714Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"\nclass config:\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    outpath = \"samples\"\n\n\nclass model_configs:\n    encodec_id = \"facebook/encodec_32khz\"\n    llama_id = \"meta-llama/Llama-3.2-1B\"\n    canary_id = \"tensorkelechi/canary_mini\"\n\n\nclass data_configs:\n    sample_rate = 32000\n    split = 1000\n    max_duration = 5\n    dtype = torch.float16\n    batch_size = 4\n    dataset_id = \"benjamin-paine/freesound-laion-640k\"\n    mini_dataset_id = \"lewtun/music_genres\"\n    processed_repo_id = \"tensorkelechi/freesound_mini\"\n\n\nclass train_configs:\n    precision = torch.float16\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    grad_steps = 4\n    epochs = 2\n    lr = 1e-4\n    sft_file = 'kaminari.safetensors'\n    model_file = 'kaminari.pth'\n    outpath = 'kaminari'","metadata":{"execution":{"iopub.status.busy":"2024-10-03T23:21:42.289661Z","iopub.execute_input":"2024-10-03T23:21:42.290113Z","iopub.status.idle":"2024-10-03T23:21:42.299387Z","shell.execute_reply.started":"2024-10-03T23:21:42.290072Z","shell.execute_reply":"2024-10-03T23:21:42.298250Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"\nmusic_prefix = \"ðŸŽ¶\"\nstart_of_music = \"<somu>\"\nend_of_music = \"<eomu>\"\nmusic_codebook_size = 2048\nmusic_codebook_num = 4\nmusic_vocab_size = 8192\n\nmusic_tokens = {\n#     \"prefix\": music_prefix,\n    \"sos\": start_of_music,\n    \"eos\": end_of_music,\n}\n\n\ndef modality_tokens_to_string(tokens):\n    \"\"\"\n    Convert audio/music tokens to a single string with prefix and postfix.\n    \"\"\"\n    prefix = music_prefix\n    start = music_tokens[\"sos\"]\n    end = music_tokens[\"eos\"]\n\n    tokens_str = []\n    # music tokens are 2-dim array\n    # Convert each token to its corresponding string representation\n    for idx in range(len(tokens[0])):\n        for layer_idx in range(len(tokens)):\n            tokens_str.append(\n                f\"<{prefix}{tokens[layer_idx][idx] + music_codebook_size * layer_idx}>\"\n            )\n\n    tokens_string = \"\".join(tokens_str)\n    tokens_string = f\"{start}{tokens_string}{end}\"\n\n    return tokens_string\n","metadata":{"execution":{"iopub.status.busy":"2024-10-03T23:21:42.302349Z","iopub.execute_input":"2024-10-03T23:21:42.302782Z","iopub.status.idle":"2024-10-03T23:21:42.312375Z","shell.execute_reply.started":"2024-10-03T23:21:42.302740Z","shell.execute_reply":"2024-10-03T23:21:42.311208Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def clear_mem():\n    torch.cuda.empty_cache()\n    gc.collect()\n\n\ndef trimpad_audio(audio):\n    samples = int(data_configs.sample_rate * data_configs.max_duration)\n#     audio = audio.numpy()\n\n    if len(audio) > samples:\n        audio = audio[:samples]\n\n    else:\n        pad_width = samples - len(audio)\n        audio = np.pad(audio, (0, pad_width), mode=\"reflect\")\n\n    return torch.as_tensor(audio)\n\n\ndef seed_everything(seed=333):\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n\nseed_everything()","metadata":{"execution":{"iopub.status.busy":"2024-10-03T23:21:42.314000Z","iopub.execute_input":"2024-10-03T23:21:42.314397Z","iopub.status.idle":"2024-10-03T23:21:42.339057Z","shell.execute_reply.started":"2024-10-03T23:21:42.314356Z","shell.execute_reply":"2024-10-03T23:21:42.337775Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nCode for audio/music tokenization and model processing\n\"\"\"\ndef prepare_tokenizer(tokenizer, tokens: list):\n    special_tokens = [f\"<{music_prefix}{x}>\" for x in range(music_vocab_size)]\n    tokenizer.add_tokens(special_tokens)\n    tokenizer.add_tokens(tokens)\n    tokenizer.add_special_tokens({'pad_token': '[pad]'})\n    \n    return tokenizer\n\n\n# encoding/compressing music/audio waveform to tokens\ndef encode_music(audio, encodec_model, audio_processor):\n    \n    audio_array = trimpad_audio(audio)\n    \n    audio_proc = audio_processor(\n        raw_audio=audio_array, sampling_rate=data_configs.sample_rate,   \n        return_tensors='pt'\n    )  # preprocess audio waveform for encoding\n#     print(len(audio_proc[\"input_values\"]))\n#     print(len(audio_proc['padding_mask']))\n    \n    masks = audio_proc[\"padding_mask\"]  # get processor masks for decoding\n\n    with torch.no_grad():\n        audio_tokens = encodec_model.encode(\n            # tokenize/encode with pretrained neural codec\n            audio_proc[\"input_values\"],\n            audio_proc[\"padding_mask\"],\n        )\n    audio_codes = audio_tokens.audio_codes\n#     print(f'audio_codes.shape ={audio_codes.shape}')\n    \n    return audio_codes[0][0], masks\n\n\ndef tokens2string(tokens):\n    \"\"\"\n    Convert visual tokens to a single string with prefix and postfix.\n    \"\"\"\n    prefix = music_prefix\n    start = music_tokens[\"sos\"]\n    end = music_tokens[\"eos\"]\n\n    # music tokens are 2-dim array\n    # Convert each token to its corresponding string representation\n    tokens_str = []\n\n    for idx in range(len(tokens[0])):\n        #         print('layer 1')\n\n        for layer_idx in range(len(tokens)):\n            #             print('layer2')\n            tokens_str.append(\n                f\"<{prefix}{tokens[layer_idx][idx] + music_codebook_size * layer_idx}>\"\n            )\n\n    tokens_string = \"\".join(tokens_str)\n    tokens_string = f\" - {start}{tokens_string}{end}\"\n    return tokens_string\n\n\n\ndef extractor2(text, tag1=start_of_music, tag2=end_of_music):\n    start = None\n    try:\n        # print(text)\n        start = text.index(tag1) + len(tag1)\n        end = text.index(tag2, start)\n        extracted_text = text[start:end].strip()\n        if not extracted_text:\n            try:\n                extracted_text = text[start:]\n            except:\n                extracted_text = text\n        return extracted_text\n    except ValueError:\n        try:\n            extracted_text = text[start:]\n        except Exception as e:\n            print(e)\n            extracted_text = text\n        return extracted_text\n\n\n# for audio decoding\ndef content2rvq_codes(content, codebook_size=2048, codebook_num=4):\n    codes = [int(code) for code in re.findall(r\"\\d+\", content)]\n    print(len(codes))  # 6004\n    codes = np.array([code % codebook_size for code in codes])\n    print(codes.shape)  # (6004,)\n    n = codes.shape[0] // codebook_num\n    print(n)  # (1501)\n    # Transpose the last two dimensions to match the desired output\n    # if can't divide evenly, drop the last few codes\n    codes = codes[: n * codebook_num]\n    print(codes.shape)\n    codes = codes.reshape(n, codebook_num).T\n    print(codes.shape)  # (4, 1501)\n    codes = np.expand_dims(codes, 0)\n    codes = np.expand_dims(codes, 0)\n    print(codes.shape)  # (1, 1, 4, 1501)\n    codes = torch.tensor(codes).long().to(config.device)\n    print(codes.shape)\n    return codes\n\n\ndef decode_music(content):\n    # codes = content2rvq_codes(content, music_codebook_size, music_codebook_num)\n    music = encodec_model.decode(content, [None])\n    print(f'decoded audio = {music.shape}')\n    music = music[0].squeeze(0).detach().cpu()\n    return music","metadata":{"execution":{"iopub.status.busy":"2024-10-03T23:21:42.340798Z","iopub.execute_input":"2024-10-03T23:21:42.341323Z","iopub.status.idle":"2024-10-03T23:21:42.366179Z","shell.execute_reply.started":"2024-10-03T23:21:42.341265Z","shell.execute_reply":"2024-10-03T23:21:42.364864Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# dataset preparation\n# for class-based music data, lewtun/music_genres\nmusic_data = load_dataset(\n    data_configs.dataset_id,\n    split=\"train\",\n    streaming=True,\n    trust_remote_code=True,\n).cast_column(\"audio\", Audio(sampling_rate=32000))\n\ndata_features = music_data.features.copy()\n\nmusic_data = music_data.map(\n    lambda r: {\"tags\": \" \".join(r[\"tags\"])}#, features=Features(data_features)\n)\n\nmusic_data = music_data.take(4000)\n\n\nmusic_data","metadata":{"execution":{"iopub.status.busy":"2024-10-03T23:21:42.367606Z","iopub.execute_input":"2024-10-03T23:21:42.368173Z","iopub.status.idle":"2024-10-03T23:21:46.686500Z","shell.execute_reply.started":"2024-10-03T23:21:42.368128Z","shell.execute_reply":"2024-10-03T23:21:46.685060Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/4.11k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7448ba8fb9d4d2281ad7b8c150248e1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Resolving data files:   0%|          | 0/1352 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c53e96bca2a6459eb63a15a44f13ec75"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Resolving data files:   0%|          | 0/123 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a1b71d197a3b4c21bc789c474cac63b2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Resolving data files:   0%|          | 0/1352 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ebf11dc8d8547e993513f7afe391591"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Resolving data files:   0%|          | 0/123 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5df20a87698544ee984812dc338daf92"}},"metadata":{}},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"IterableDataset({\n    features: Unknown,\n    n_shards: 1352\n})"},"metadata":{}}]},{"cell_type":"code","source":"# Audio encoder, FaceBook Encodec-32khz\nencodec_model = EncodecModel.from_pretrained(model_configs.encodec_id)\n\naudio_processor = AutoProcessor.from_pretrained(\n    model_configs.encodec_id\n)  # preprocessor for neural audio codec\n\n# freeze or prevent gradient update\nencodec_model=encodec_model.eval()\ntype(encodec_model)","metadata":{"execution":{"iopub.status.busy":"2024-10-03T23:21:46.688098Z","iopub.execute_input":"2024-10-03T23:21:46.688554Z","iopub.status.idle":"2024-10-03T23:21:51.821792Z","shell.execute_reply.started":"2024-10-03T23:21:46.688505Z","shell.execute_reply":"2024-10-03T23:21:51.820599Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/758 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"449130ed98804429b8e64a5152b32b2a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/236M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2bac2525d4c4ea8bc312eec2d6825df"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n  WeightNorm.apply(module, name, dim)\n/opt/conda/lib/python3.10/site-packages/transformers/models/encodec/modeling_encodec.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  self.register_buffer(\"padding_total\", torch.tensor(kernel_size - stride, dtype=torch.int64), persistent=False)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/234 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"00dc0a739987421eafc8c90a3e14c56c"}},"metadata":{}},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"transformers.models.encodec.modeling_encodec.EncodecModel"},"metadata":{}}]},{"cell_type":"code","source":"class MusicData(IterableDataset):\n    def __init__(self, tokenizer, dataset=music_data):\n        self.dataset = dataset\n        self.tokenizer = tokenizer\n\n    def __len__(self):\n        return data_configs.split\n\n    def __iter__(self):\n        for sample in self.dataset:\n            audio_tokens = encode_music(\n                sample[\"audio\"][\"array\"],\n                encodec_model=encodec_model,\n                audio_processor=audio_processor,\n            )\n            audio_string = tokens2string(audio_tokens[0])\n\n            label = sample[\"tags\"]#' '.join(sample[\"tags\"])\n            data_string = label + audio_string\n\n            input_tokens = self.tokenizer(data_string, return_tensors='pt', truncation=True, padding='max_length', max_length=1024)\n            token_ids = input_tokens[\"input_ids\"]\n            attn_mask = input_tokens[\"attention_mask\"]\n\n            yield {\"input_ids\": token_ids, \"attention_mask\": attn_mask}","metadata":{"execution":{"iopub.status.busy":"2024-10-03T23:21:51.823553Z","iopub.execute_input":"2024-10-03T23:21:51.824070Z","iopub.status.idle":"2024-10-03T23:21:51.834753Z","shell.execute_reply.started":"2024-10-03T23:21:51.824017Z","shell.execute_reply":"2024-10-03T23:21:51.833527Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"from transformers import LlamaTokenizer\n\ntokenizer = LlamaTokenizer.from_pretrained(\"hf-internal-testing/llama-tokenizer\")\n\n# LLM tokenizer\n# tokenizer = AutoTokenizer.from_pretrained(\n# #     model_configs.llama_id,\n# )\n# # tokenizer = prepare_tokenizer(tokenizer)\n\n# Llama model architecture, for initial experiments\nllama_config = LlamaConfig(\n    num_attention_heads=16,\n    num_hidden_layers=8,\n    num_key_value_heads=4,\n    hidden_size=1024,\n    intermediate_size=4096,\n#     head_dim=32,\n)\n\ntiny_llama = LlamaModel(config=llama_config)\ntiny_llama.config","metadata":{"execution":{"iopub.status.busy":"2024-10-03T23:21:51.839398Z","iopub.execute_input":"2024-10-03T23:21:51.839780Z","iopub.status.idle":"2024-10-03T23:21:58.106787Z","shell.execute_reply.started":"2024-10-03T23:21:51.839739Z","shell.execute_reply":"2024-10-03T23:21:58.105196Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.54k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f88fb2532484419843a7d6da08047e8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bdabfb908f3f4b2a8fd7c8bcf1b987cc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/411 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd85b1fd6fe74dcb8e52576645203546"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"962e46aaad3d4eda8f9463793da822b5"}},"metadata":{}},{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"LlamaConfig {\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 1,\n  \"eos_token_id\": 2,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 1024,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 4096,\n  \"max_position_embeddings\": 2048,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 16,\n  \"num_hidden_layers\": 8,\n  \"num_key_value_heads\": 4,\n  \"pretraining_tp\": 1,\n  \"rms_norm_eps\": 1e-06,\n  \"rope_scaling\": null,\n  \"rope_theta\": 10000.0,\n  \"tie_word_embeddings\": false,\n  \"transformers_version\": \"4.44.2\",\n  \"use_cache\": true,\n  \"vocab_size\": 32000\n}"},"metadata":{}}]},{"cell_type":"code","source":"tokens = list(music_tokens.values())\n\ntokenizer = prepare_tokenizer(tokenizer, tokens)\ntiny_llama.resize_token_embeddings(len(tokenizer))\ntiny_llama.lm_head = nn.Linear(llama_config.hidden_size, len(tokenizer), bias=False)\ntype(tiny_llama), tiny_llama.config","metadata":{"execution":{"iopub.status.busy":"2024-10-03T23:21:58.120675Z","iopub.execute_input":"2024-10-03T23:21:58.121334Z","iopub.status.idle":"2024-10-03T23:22:00.497481Z","shell.execute_reply.started":"2024-10-03T23:21:58.121262Z","shell.execute_reply":"2024-10-03T23:22:00.496182Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"(transformers.models.llama.modeling_llama.LlamaModel,\n LlamaConfig {\n   \"attention_bias\": false,\n   \"attention_dropout\": 0.0,\n   \"bos_token_id\": 1,\n   \"eos_token_id\": 2,\n   \"hidden_act\": \"silu\",\n   \"hidden_size\": 1024,\n   \"initializer_range\": 0.02,\n   \"intermediate_size\": 4096,\n   \"max_position_embeddings\": 2048,\n   \"mlp_bias\": false,\n   \"model_type\": \"llama\",\n   \"num_attention_heads\": 16,\n   \"num_hidden_layers\": 8,\n   \"num_key_value_heads\": 4,\n   \"pretraining_tp\": 1,\n   \"rms_norm_eps\": 1e-06,\n   \"rope_scaling\": null,\n   \"rope_theta\": 10000.0,\n   \"tie_word_embeddings\": false,\n   \"transformers_version\": \"4.44.2\",\n   \"use_cache\": true,\n   \"vocab_size\": 40195\n })"},"metadata":{}}]},{"cell_type":"code","source":"dset = MusicData(tokenizer)\nmini_train_loader = DataLoader(dataset=dset, batch_size=data_configs.batch_size)\n\nx_sample = next(iter(mini_train_loader))\n# x_sample/","metadata":{"execution":{"iopub.status.busy":"2024-10-03T23:22:00.581297Z","iopub.execute_input":"2024-10-03T23:22:00.581720Z","iopub.status.idle":"2024-10-03T23:22:25.828957Z","shell.execute_reply.started":"2024-10-03T23:22:00.581672Z","shell.execute_reply":"2024-10-03T23:22:25.827656Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"audio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\n","output_type":"stream"}]},{"cell_type":"code","source":"# training definitions\nmodel = tiny_llama\n\nloss_fn = nn.CrossEntropyLoss(reduction=\"none\", ignore_index=tokenizer.pad_token_id)  # loss function\noptimizer = optim.AdamW(model.parameters(), lr=train_configs.lr)\nscheduler = lr_scheduler.CosineAnnealingWarmRestarts(\n    optimizer,\n    T_0=1000,  # restart every 1000 steps\n    T_mult=1\n)\n\nscaler = GradScaler()\n\n# configure accelerate\n# accelerator = Accelerator()\n# model, mini_train_loader, optimizer, scheduler = accelerator.prepare(\n#     # cofnigure modules for training\n#     model,\n#     mini_train_loader,\n#     optimizer,\n#     scheduler,\n# )","metadata":{"execution":{"iopub.status.busy":"2024-10-03T23:22:25.841914Z","iopub.execute_input":"2024-10-03T23:22:25.842309Z","iopub.status.idle":"2024-10-03T23:22:25.853990Z","shell.execute_reply.started":"2024-10-03T23:22:25.842271Z","shell.execute_reply":"2024-10-03T23:22:25.852787Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_30/2665247093.py:12: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = GradScaler()\n/opt/conda/lib/python3.10/site-packages/torch/amp/grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"gen_configs = {\n    \"temperature\": 1,\n    \"top_p\": 1.0,\n    \"top_k\": 200,\n    \"do_sample\": True,\n    \"max_new_tokens\": 2000,\n    \"min_new_tokens\": 10,\n    \"repetition_penalty\": 1.15,\n}\n\n\ndef _postprocess(input):\n    extract = extractor2(input)\n    print('extract')\n    print(extract)\n    reconstruct_codes = content2rvq_codes(extract)\n    print(f'recoded {reconstruct_codes.shape}')\n    waveform = decode_music(reconstruct_codes)\n\n    waveform = waveform[0].squeeze(0).detach().cpu()\n\n    return waveform\n\n\n@torch.no_grad()\ndef bird_call(\n    prompt, model, tokenizer\n):  # prompt might be just a class/single word/description for v1\n    input = tokenizer.encode(prompt, return_tensors=\"pt\")\n    input_ids = input.input_ids\n    gen_tokens = model.generate(\n        **input_ids.to(config.device), generation_config=gen_configs\n    )\n    tokens = tokenizer.batch_decode(gen_tokens.sequences.cpu(), skip_special_tokens=True)\n\n    output = _postprocess(tokens[0])\n    print(f'postprocessed: {output}')\n    return output","metadata":{"execution":{"iopub.status.busy":"2024-10-03T23:22:25.855469Z","iopub.execute_input":"2024-10-03T23:22:25.855824Z","iopub.status.idle":"2024-10-03T23:22:25.866459Z","shell.execute_reply.started":"2024-10-03T23:22:25.855787Z","shell.execute_reply":"2024-10-03T23:22:25.865241Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# model.set_output_embedding(model.lm_head)\n\nmodel","metadata":{"execution":{"iopub.status.busy":"2024-10-03T23:32:10.129437Z","iopub.execute_input":"2024-10-03T23:32:10.129943Z","iopub.status.idle":"2024-10-03T23:32:10.139864Z","shell.execute_reply.started":"2024-10-03T23:32:10.129900Z","shell.execute_reply":"2024-10-03T23:32:10.138458Z"},"trusted":true},"execution_count":34,"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"LlamaModel(\n  (embed_tokens): Embedding(40195, 1024)\n  (layers): ModuleList(\n    (0-7): 8 x LlamaDecoderLayer(\n      (self_attn): LlamaSdpaAttention(\n        (q_proj): Linear(in_features=1024, out_features=1024, bias=False)\n        (k_proj): Linear(in_features=1024, out_features=256, bias=False)\n        (v_proj): Linear(in_features=1024, out_features=256, bias=False)\n        (o_proj): Linear(in_features=1024, out_features=1024, bias=False)\n        (rotary_emb): LlamaRotaryEmbedding()\n      )\n      (mlp): LlamaMLP(\n        (gate_proj): Linear(in_features=1024, out_features=4096, bias=False)\n        (up_proj): Linear(in_features=1024, out_features=4096, bias=False)\n        (down_proj): Linear(in_features=4096, out_features=1024, bias=False)\n        (act_fn): SiLU()\n      )\n      (input_layernorm): LlamaRMSNorm((1024,), eps=1e-06)\n      (post_attention_layernorm): LlamaRMSNorm((1024,), eps=1e-06)\n    )\n  )\n  (norm): LlamaRMSNorm((1024,), eps=1e-06)\n  (rotary_emb): LlamaRotaryEmbedding()\n  (lm_head): Linear(in_features=1024, out_features=40195, bias=False)\n)"},"metadata":{}}]},{"cell_type":"code","source":"def count_params(model: nn.Module):\n    p_count = sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n    return p_count\n\n\nprint(f\"model parameters (training) = {count_params(model)}\")\n\ndef clearmem():\n    torch.cuda.empty_cache()\n    gc.collect()\n\ndef logger(model) -> None:\n    wandb.login(key=wkey)\n    wandb.init(project=\"kaminari_v1\", name=\"audiogen-1-sandbox-8k\")\n    wandb.watch(model)\n\nlogger(model)\n\n\n@torch.no_grad\ndef epoch_sample(model: LlamaModel = model, prompt_class=\"classical\"):\n    sample_tokens = bird_call(prompt, model, tokenizer)\n    now = datetime.datetime.now()\n    filename = now.strftime(\"%m%d_%H%M%S\") + \".wav\"\n    file_name = os.path.join(config.outpath, filename)\n    print(\"saved: \", file_name)\n    torchaudio.save(file_name, sample_tokens, data_configs.sample_rate)\n\n    return filename\n","metadata":{"execution":{"iopub.status.busy":"2024-10-03T23:22:25.890025Z","iopub.execute_input":"2024-10-03T23:22:25.890435Z","iopub.status.idle":"2024-10-03T23:22:25.902880Z","shell.execute_reply.started":"2024-10-03T23:22:25.890390Z","shell.execute_reply":"2024-10-03T23:22:25.901790Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"model parameters (training) = 203971584\n","output_type":"stream"}]},{"cell_type":"code","source":"clearmem()","metadata":{"execution":{"iopub.status.busy":"2024-10-03T23:22:25.904498Z","iopub.execute_input":"2024-10-03T23:22:25.904918Z","iopub.status.idle":"2024-10-03T23:22:26.376030Z","shell.execute_reply.started":"2024-10-03T23:22:25.904853Z","shell.execute_reply":"2024-10-03T23:22:26.374796Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"CUDA_LAUNCH_BLOCKING=1\nTORCH_USE_CUDA_DSA=True\n\nimport torch._dynamo\ntorch._dynamo.config.suppress_errors = True","metadata":{"execution":{"iopub.status.busy":"2024-10-03T23:22:26.402410Z","iopub.execute_input":"2024-10-03T23:22:26.402844Z","iopub.status.idle":"2024-10-03T23:22:26.411586Z","shell.execute_reply.started":"2024-10-03T23:22:26.402804Z","shell.execute_reply":"2024-10-03T23:22:26.410331Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# model=model.to(train_configs.device)","metadata":{"execution":{"iopub.status.busy":"2024-10-03T23:22:26.415678Z","iopub.execute_input":"2024-10-03T23:22:26.416144Z","iopub.status.idle":"2024-10-03T23:22:26.423100Z","shell.execute_reply.started":"2024-10-03T23:22:26.416102Z","shell.execute_reply":"2024-10-03T23:22:26.421958Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# xc = torch.randn(4, 1, 1024).to(config.device)\n\n# out = model(input_ids=x_sample['input_ids'].long().squeeze().to(train_configs.device))\n# out","metadata":{"execution":{"iopub.status.busy":"2024-10-03T23:22:26.424537Z","iopub.execute_input":"2024-10-03T23:22:26.424969Z","iopub.status.idle":"2024-10-03T23:22:26.433224Z","shell.execute_reply.started":"2024-10-03T23:22:26.424921Z","shell.execute_reply":"2024-10-03T23:22:26.432126Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"print(f\"Vocabulary size: {model.config.vocab_size}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-03T23:22:26.434533Z","iopub.execute_input":"2024-10-03T23:22:26.434907Z","iopub.status.idle":"2024-10-03T23:22:26.449780Z","shell.execute_reply.started":"2024-10-03T23:22:26.434850Z","shell.execute_reply":"2024-10-03T23:22:26.448501Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"Vocabulary size: 40195\n","output_type":"stream"}]},{"cell_type":"code","source":"model.resize_token_embeddings(len(tokenizer))\nmodel.config.vocab_size = len(tokenizer)\n\nlen(tokenizer)","metadata":{"execution":{"iopub.status.busy":"2024-10-03T23:33:23.450553Z","iopub.execute_input":"2024-10-03T23:33:23.451115Z","iopub.status.idle":"2024-10-03T23:33:23.461627Z","shell.execute_reply.started":"2024-10-03T23:33:23.451066Z","shell.execute_reply":"2024-10-03T23:33:23.460087Z"},"trusted":true},"execution_count":35,"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"40195"},"metadata":{}}]},{"cell_type":"code","source":"tokenizer.pad_token_id","metadata":{"execution":{"iopub.status.busy":"2024-10-03T23:22:26.471298Z","iopub.execute_input":"2024-10-03T23:22:26.471692Z","iopub.status.idle":"2024-10-03T23:22:26.479607Z","shell.execute_reply.started":"2024-10-03T23:22:26.471646Z","shell.execute_reply":"2024-10-03T23:22:26.478046Z"},"trusted":true},"execution_count":28,"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"40194"},"metadata":{}}]},{"cell_type":"code","source":"print(f\"Tokenizer vocabulary size: {len(tokenizer)}\")\nprint(f\"Model embedding size: {model.embed_tokens.num_embeddings}\")\nprint(f\"Model config vocab size: {model.config.vocab_size}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-03T23:34:17.189618Z","iopub.execute_input":"2024-10-03T23:34:17.190113Z","iopub.status.idle":"2024-10-03T23:34:17.198116Z","shell.execute_reply.started":"2024-10-03T23:34:17.190070Z","shell.execute_reply":"2024-10-03T23:34:17.196440Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"Tokenizer vocabulary size: 40195\nModel embedding size: 40195\nModel config vocab size: 40195\n","output_type":"stream"}]},{"cell_type":"code","source":"print(f\"Tokenizer vocabulary size: {len(tokenizer)}\")\nprint(f\"Model vocabulary size: {model.config.vocab_size}\")\nassert len(tokenizer) == model.config.vocab_size, \"Tokenizer and model vocabulary sizes should match\"","metadata":{"execution":{"iopub.status.busy":"2024-10-03T23:22:26.481356Z","iopub.execute_input":"2024-10-03T23:22:26.481794Z","iopub.status.idle":"2024-10-03T23:22:26.495222Z","shell.execute_reply.started":"2024-10-03T23:22:26.481752Z","shell.execute_reply":"2024-10-03T23:22:26.493850Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"Tokenizer vocabulary size: 40195\nModel vocabulary size: 40195\n","output_type":"stream"}]},{"cell_type":"code","source":"def trainer(\n    model=model, train_loader=mini_train_loader, epoch_count=train_configs.epochs\n):\n    model.train()\n    model.to(config.device)\n\n    train_loss = 0.0\n    # training loop\n    for epoch in tqdm(range(epoch_count)):\n        print(f'training for epoch {epoch+1}')\n        start_time = time()\n        optimizer.zero_grad()  # clear gradient graph\n\n        for step, batch in tqdm(enumerate(train_loader)):\n            optimizer.zero_grad()  # clear gradient graph\n\n            input_tokens = batch[\"input_ids\"].to(config.device)\n            attn_mask = batch[\"attention_mask\"].to(config.device)\n\n            assert input_tokens.max() < model.config.vocab_size, f\"Input contains token ID {input_tokens.max().item()} which is >= vocab size {model.config.vocab_size}\"\n            # Mixed precision training\n            with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n                outputs = model(\n                    input_ids=input_tokens.long().squeeze(),  # .squeeze(),\n                    attention_mask=attn_mask.long().squeeze(),  # .squeeze(),\n#                     labels=input_tokens.long().squeeze(),\n                )[0]\n                outputs = model.lm_head(outputs)\n\n                # clear memory\n                clearmem()\n\n                # slice tensors, due to 'next-token prediction' objective\n                # all except last token\n                output_tensor = outputs[..., :-1, :].contiguous()\n                # all except the first token\n                targets = input_tokens[..., 1:].contiguous()\n                shift_mask = attn_mask[..., 1:].contiguous()\n\n                model_output = output_tensor.view(-1, output_tensor.size(-1))\n                targets = targets.view(-1)\n\n                # compute loss for step\n                step_loss = loss_fn(model_output, targets)\n                clearmem()\n                \n                total_tokens = shift_mask.sum()\n                step_loss = step_loss.sum() / (total_tokens + 1e-8)\n                \n                # Scale loss by accumulation steps\n                train_loss = step_loss / train_configs.grad_steps  # Normalize the loss\n                \n                print(f\"step {step}: loss {step_loss:.4f}\")\n                clearmem()                \n            # optimizer.step()\n\n            # Scales loss. Calls backward() on scaled loss to create scaled gradients.\n            scaler.scale(train_loss).backward()\n\n            clearmem()\n\n            if (step + 1) % train_configs.grad_steps == 0:\n                # Unscales the gradients of optimizer's assigned params in-place\n                scaler.step(optimizer)\n                # Updates the scale for next iteration\n                scaler.update()\n                optimizer.zero_grad()\n\n            if step % 5 == 0:\n                wandb.log({\"train_loss\": train_loss})\n            \n            if (step % 100) == 0:\n                checkpoint = {\n                    \"epoch\": epoch,\n                    \"model_state_dict\": model.state_dict(),\n                    \"optimizer_state_dict\": optimizer.state_dict(),\n                    \"scheduler_state\": scheduler.state_dict(),\n                    \"loss\": train_loss,\n                }\n\n                # save checkpoint\n                torch.save(checkpoint, f\"kaminari_mini_check_{epoch}.pth\")\n                # log audio sample to WandB\n                try:\n                    test_sample_file = epoch_sample(model)\n                    wandb.log(\n                        {\n                            \"audio_sample\": wandb.Audio(\n                                test_sample_file,\n                                caption=f\"test_audio_track_{step}\",\n                                sample_rate=data_configs.sample_rate,\n                            )\n                        }\n                    )\n                except Exception as e:\n                    print(f'error logging sample: {e}')\n                \n        scheduler.step()\n\n        gc.collect()\n        epoch_time = time() - start_time\n\n        print(f\"Epoch {epoch} of {epoch_count}, train_loss: {train_loss:.4f}\")\n\n        print(f\"Epoch @ {epoch} complete in {epoch_time}!\")\n\n    print(f\"End metrics for run of {epoch_count}, train_loss: {train_loss:.4f}\")\n\n    save_model(model, train_configs.sft_file)  # save to .safetensors file\n\n    torch.save(model.state_dict(), f\"{train_configs.model_file}\")\n\ntrainer()","metadata":{"execution":{"iopub.status.busy":"2024-10-04T00:11:34.001744Z","iopub.execute_input":"2024-10-04T00:11:34.002786Z","iopub.status.idle":"2024-10-04T00:41:04.213645Z","shell.execute_reply.started":"2024-10-04T00:11:34.002742Z","shell.execute_reply":"2024-10-04T00:41:04.211587Z"},"trusted":true},"execution_count":54,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3589e29c364d4b27ad2cdd9f42953eca"}},"metadata":{}},{"name":"stdout","text":"training for epoch 1\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee5ed9ca91c74213af34fffdc552e070"}},"metadata":{}},{"name":"stdout","text":"audio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\nstep 0: loss 10.696185111999512\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\nstep 1: loss 10.750092506408691\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\nstep 2: loss 10.599929809570312\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\nstep 3: loss 10.716127395629883\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\nstep 4: loss 10.277107238769531\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\nstep 5: loss 10.559674263000488\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\nstep 6: loss 10.526268005371094\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\nstep 7: loss 9.963801383972168\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\nstep 8: loss 7.973994731903076\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\nstep 9: loss 10.1277437210083\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\nstep 10: loss 9.280688285827637\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\nstep 11: loss 9.342376708984375\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\nstep 12: loss 7.567083358764648\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\nstep 13: loss 10.180267333984375\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\nstep 14: loss 10.515371322631836\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\nstep 15: loss 8.539643287658691\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\nstep 16: loss 8.601490020751953\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\nstep 17: loss 10.661795616149902\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\nstep 18: loss 7.352926254272461\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\nstep 19: loss 8.988655090332031\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\nstep 20: loss 9.36124038696289\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\nstep 21: loss 8.46739387512207\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\nstep 22: loss 8.741004943847656\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\nstep 23: loss 9.6987886428833\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\nstep 24: loss 10.099658966064453\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\nstep 25: loss 8.305779457092285\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\nstep 26: loss 7.526179313659668\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\nstep 27: loss 8.806510925292969\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\nstep 28: loss 9.469889640808105\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\nstep 29: loss 10.055397987365723\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\nstep 30: loss 8.301277160644531\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\nstep 31: loss 6.446662425994873\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\nstep 32: loss 8.210050582885742\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\nstep 33: loss 6.403970718383789\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\nstep 34: loss 8.531370162963867\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\nstep 35: loss 9.176823616027832\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\nstep 36: loss 8.78394603729248\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\naudio_codes.shape =torch.Size([1, 1, 4, 250])\nstep 37: loss 10.165579795837402\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[54], line 109\u001b[0m\n\u001b[1;32m    105\u001b[0m     save_model(model, train_configs\u001b[38;5;241m.\u001b[39msft_file)  \u001b[38;5;66;03m# save to .safetensors file\u001b[39;00m\n\u001b[1;32m    107\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_configs\u001b[38;5;241m.\u001b[39mmodel_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 109\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[54], line 57\u001b[0m, in \u001b[0;36mtrainer\u001b[0;34m(model, train_loader, epoch_count)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# optimizer.step()\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Scales loss. Calls backward() on scaled loss to create scaled gradients.\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m clearmem()\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m train_configs\u001b[38;5;241m.\u001b[39mgrad_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;66;03m# Unscales the gradients of optimizer's assigned params in-place\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/graph.py:768\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    766\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"\n# def trainer_wrapper(train_function=):\n#     train_function()\n\n\n# notebook_launcher(trainer_wrapper, num_processes=2)","metadata":{"execution":{"iopub.status.busy":"2024-10-03T23:22:46.099687Z","iopub.status.idle":"2024-10-03T23:22:46.100354Z","shell.execute_reply.started":"2024-10-03T23:22:46.100026Z","shell.execute_reply":"2024-10-03T23:22:46.100057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clearmem()","metadata":{"execution":{"iopub.status.busy":"2024-10-03T23:56:03.558319Z","iopub.execute_input":"2024-10-03T23:56:03.558807Z","iopub.status.idle":"2024-10-03T23:56:04.249198Z","shell.execute_reply.started":"2024-10-03T23:56:03.558754Z","shell.execute_reply":"2024-10-03T23:56:04.247819Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-10-03T23:22:46.104604Z","iopub.status.idle":"2024-10-03T23:22:46.105212Z","shell.execute_reply.started":"2024-10-03T23:22:46.104904Z","shell.execute_reply":"2024-10-03T23:22:46.104940Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}